# cronjob-enterprise-crawlers.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: enterprise-crawlers
spec:
  schedule: "5 2 * * *"
  timeZone: "Asia/Seoul" # (K8s 1.27+)
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  startingDeadlineSeconds: 1800
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 3600 # 1 hour
      backoffLimit: 0
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: crawlers
              image: leosunghojung/ilhaeng-crawling:df70995
              command: ["sh", "-c"]
              args:
                - >
                  python run_all.py --include '*-r.py' --include '*-oa.py' --exclude __init__.py --concurrency ${CONCURRENCY:-3} --timeout ${TIMEOUT:-2400} --retries ${RETRIES:-1} --pages ${PAGES:-1-3} --detail-delay ${DETAIL_DELAY:-0.3} --workers ${WORKERS:-6} --ncp-prefix ${NCP_DEFAULT_DIR:-prod}

              envFrom:
                - configMapRef:
                    name: crawler-config
              env:
                - name: PRESIGN_AUTH
                  valueFrom:
                    secretKeyRef:
                      name: crawler-secret
                      key: presign-auth
              resources:
                requests:
                  cpu: "500m"
                  memory: "1Gi"
                limits:
                  cpu: "2"
                  memory: "4Gi"
              volumeMounts:
                - name: out
                  mountPath: /data/out
          volumes:
            - name: out
              emptyDir: {}
